from math import sqrt
from random import random, uniform, normalvariate
from activation_functions import KNOWN_AFD

class Network:
    """
    A feed-forward neural network.
    """
    def __init__(self, size:list, activation_functions:list):
        """
        The constructor for Network class.

        Parameters
        ----------
        size : list of int
            [number of inputs, ... , neurons in each hidden layer, ... ,
              neurons in output layer]\n
            Example: [3, 4, 4, 2]
        activation functions : list of str
            [activation functions for each hidden layer, ... ,
            activation functions for the ouput layer]\n
            Example: ["tanh", "sigmoid", "linear"]
        """
        self.size = size
        self.layers = []
        for i in range(1, len(size)):
            self.layers.append(Layer(size[i], size[i - 1], activation_functions[i - 1]))

    def initialize(self, initialization_methods:list):
        """
        Initializes the weights of the neural network.

        Parameters
        ----------
        initialization methods : list of str
            [methods for hidden layers, ... , method for output layer]\n
            Example: ["glorot", "glorot", "he"]
        """
        for layer, method in zip(self.layers, initialization_methods):
            initialization = KNOWN_WI[method]
            initialization(layer)

    def feedforward(self, inputs:list):
        """
        Get output generated by the neural network.

        Also updates the previous activations of each layer.
        
        Parameters
        ----------
        inputs : list of float
            Inputs.

        Returns
        -------
        list of float
            Outputs.
        """
        last = inputs
        for layer in self.layers:
            last = layer.activations(last)
        return last

    def reverse(self, inputs:list, expected_outputs:list, learning_rate:float):
        """
        Train the network.

        To be used after a feed-forward step.

        Parameters
        ----------
        inputs : list of float
        expected outputs : list of float
        learning rate : float
        """
        if len(self.layers) == 1:
            self.one_layer_reverse(inputs, expected_outputs, learning_rate)
        else:
            self.outputlayer_reverse(expected_outputs, learning_rate)
            self.hidden_layers_reverse(learning_rate)
            self.first_layer_reverse(inputs, learning_rate)

    def loss(self, expected_output:list):
        """
        Get mean squared error of the networks prediction against expected output.

        To be used after a feed-forward step.

        Parameters
        ----------
        expected outputs : list of float

        Returns
        -------
        float
            Error
        """
        error = 0
        output = self.layers[-1].previous_activations
        for output, expected in zip(output, expected_output):
            error += (output - expected) ** 2
        n = len(expected_output)
        return error / n

    def one_layer_reverse(self, inputs:list, expected_outputs:list, learning_rate:float):
        layer = self.layers[0]
        layer.update_weights_and_bias(inputs, expected_outputs, learning_rate)
        layer.reset_all()

    def outputlayer_reverse(self, expected_outputs:list, learning_rate:float):
        layer = self.layers[-1]
        derivatives = layer.previous_layer_derivatives(expected_outputs)
        last_layer = self.layers[-2]
        last_layer.set_derivatives_of_cost(derivatives)
        last_layer_activations = last_layer.get_previous_activations()
        layer.update_weights_and_bias(last_layer_activations, expected_outputs, learning_rate)
        layer.reset_all()

    def hidden_layers_reverse(self, learning_rate:float):
        for i in range(len(self.layers) - 2, 0, -1):
            layer = self.layers[i]
            expected = layer.get_expected()
            derivatives = layer.previous_layer_derivatives(expected)
            last_layer = self.layers[i - 1]
            last_layer_activations = last_layer.get_previous_activations()
            last_layer.set_derivatives_of_cost(derivatives)
            layer.update_weights_and_bias(last_layer_activations, expected, learning_rate)
            layer.reset_all()

    def first_layer_reverse(self, inputs:list, learning_rate:float):
        layer = self.layers[0]
        last_layer_activations = inputs
        expected = layer.get_expected()
        layer.update_weights_and_bias(last_layer_activations, expected, learning_rate)
        layer.reset_all()

    def as_dict(self):
        """
        Get neural network in a dictionary.
        """
        self_dict = {"size": self.size}
        for i, layer in enumerate(self.layers):
            self_dict[i] = []
            for neuron in layer.neurons:
                self_dict[i].append(neuron.weights + [neuron.bias] +
                                    [str(neuron.activation_function.__name__)])
        return self_dict

    def __repr__(self):
        return f"Network {self.layers}"

class Layer:
    """
    A layer of a neural network.
    """
    def __init__(self, size:int, previous_layer_size:int, activation_function:str):
        self.neurons = []
        for _ in range(size):
            self.neurons.append(Neuron(previous_layer_size, activation_function))
        self.previous_activations = None

    def activations(self, previous_layer_outputs:list):
        self.previous_activations = []
        for neuron in self.neurons:
            self.previous_activations.append(neuron.activation(previous_layer_outputs))
        return self.previous_activations

    def get_previous_activations(self):
        return self.previous_activations

    def get_expected(self):
        expected = []
        for neuron in self.neurons:
            expected.append(neuron.previous_activation - neuron.derivative_of_cost)
        return expected

    def set_derivatives_of_cost(self, derivatives:list):
        for neuron, derivative in zip(self.neurons, derivatives):
            neuron.set_derivative_of_cost(derivative)

    def previous_layer_derivatives(self, expected:list):
        derivatives_by_neuron = []
        for value, neuron in zip(expected, self.neurons):
            derivatives_by_neuron.append(neuron.previous_layer_derivatives(value))
        derivatives = []
        for i in range(len(derivatives_by_neuron[0])):
            derivatives.append(0)
            for j in range(len(derivatives_by_neuron)):
                derivatives[i] += derivatives_by_neuron[j][i]
        return derivatives

    def update_weights_and_bias(self, previous_layer_activations:list,
                                expected:list, learning_rate:float):
        for neuron, value in zip(self.neurons, expected):
            weight_derivatives = neuron.weights_derivatives(previous_layer_activations, value)
            bias_derivative = neuron.bias_derivative(value)
            neuron.update_weights_and_bias(weight_derivatives, bias_derivative, learning_rate)

    def reset_all(self):
        for neuron in self.neurons:
            neuron.reset_all()

    def __repr__(self):
        return f"Layer: {self.neurons}"

class Neuron:
    """
    A neuron of a neural netwok.
    """
    def __init__(self, previous_layer_neurons, activation_function:str):
        if activation_function not in KNOWN_AFD:
            raise ValueError("Not known activation function")
        self.activation_function = KNOWN_AFD[activation_function][0]
        self.activation_function_derivative = KNOWN_AFD[activation_function][1]
        self.weights = []
        for _ in range(previous_layer_neurons):
            self.weights.append(0)
        self.bias = 0
        self.derivative_of_cost = None
        self.previous_weighted = None
        self.previous_activation = None

    def activation(self, previous_layer_output:list):
        self.previous_weighted = 0
        for weight, output in zip(self.weights, previous_layer_output):
            self.previous_weighted += weight * output
        self.previous_weighted += self.bias
        self.previous_activation = self.activation_function(self.previous_weighted)
        return self.previous_activation

    def previous_layer_derivatives(self, expected:float):
        derivatives = []
        part = self.activation_function_derivative(self.previous_weighted)
        part *= 2 * (self.previous_activation - expected)
        for weight in self.weights:
            derivatives.append(weight * part)
        return derivatives

    def weights_derivatives(self, previous_layer_activations:list, expected:float):
        derivatives = []
        part = self.activation_function_derivative(self.previous_weighted)
        part *= 2 * (self.previous_activation - expected)
        for activation in previous_layer_activations:
            derivatives.append(- activation * part)
        return derivatives

    def bias_derivative(self, expected:float):
        part = self.activation_function_derivative(self.previous_weighted)
        part *= 2 * (self.previous_activation - expected)
        return - part

    def update_derivative_of_cost(self, new:float):
        self.derivative_of_cost = new

    def update_weights_and_bias(self, delta_weights:list, delta_bias:float, learning_rate:float):
        updated_weights = []
        for old, new in zip(self.weights, delta_weights):
            updated_weights.append(old + new * learning_rate)
        self.weights = updated_weights
        self.bias += delta_bias * learning_rate

    def reset_all(self):
        self.previous_activation = None
        self.derivative_of_cost = None
        self.previous_weighted = None

    def set_derivative_of_cost(self, derivative:float):
        self.derivative_of_cost = derivative

    def set_weights_and_bias(self, weights:list, bias:float):
        self.weights = weights
        self.bias = bias

    def __repr__(self):
        weights = [round(weight, 3) for weight in self.weights]
        bias = [round(self.bias, 3)]
        return f"Neuron {weights + bias}"

class CustomNetwork(Network):
    """
    A previously stored feed-forward neural network.
    """
    def __init__(self, network:dict):
        """
        The constructor for CustomNetwork class.

        Parameters
        ----------
        network : dict
            A dictionary containing the information about the network.\n
            e.g. {  "size": [2, 3, 2],
                    "0":    [[weight, weight, bias, activation function],
                            [weight, weight, bias, activation function],
                            [weight, weight, bias, activation function]],
                    "1":    [[weight, weight, weight, bias, activation function],
                            [weight, weight, weight, bias, activation function]] }
        """
        self.size = network["size"]
        self.layers = []
        for i in range(len(self.size) - 1):
            layer = network[str(i)]
            neurons = []
            for neuron in layer:
                weights = neuron[:-2]
                bias = neuron[-2]
                activation_function = neuron[-1]
                neurons.append(CustomNeuron(weights, bias, activation_function))
            self.layers.append(CustomLayer(neurons))

class CustomLayer(Layer):
    def __init__(self, neurons:list):
        self.neurons = neurons
        self.previous_activations = None

class CustomNeuron(Neuron):
    def __init__(self, weights:list, bias:float, activation_function:str):
        self.activation_function = KNOWN_AFD[activation_function][0]
        self.activation_function_derivative = KNOWN_AFD[activation_function][1]
        self.weights = weights
        self.bias = bias
        self.derivative_of_cost = None
        self.previous_weighted = None
        self.previous_activation = None

# Weight initializations
def basic(layer:Layer):
    for neuron in layer.neurons:
        new = [random() - 0.5 for _ in range(len(neuron.weights))]
        neuron.set_weights_and_bias(new, 0)

def glorot(layer:Layer):
    """To be used with sigmoid and tanh"""
    fan_in = len(layer.neurons[0].weights)
    fan_out = len(layer.neurons)
    boundary = sqrt(6) / sqrt(fan_in + fan_out)
    for neuron in layer.neurons:
        new = [uniform(-boundary, boundary) for _ in range(len(neuron.weights))]
        neuron.set_weights_and_bias(new, 0)

def kaiming(layer:Layer):
    """To be used with ReLU"""
    n = len(layer.neurons[0].weights)
    deviation = sqrt(2 / n)
    for neuron in layer.neurons:
        new = [abs(normalvariate(sigma=deviation)) for _ in range(len(neuron.weights))]
        neuron.set_weights_and_bias(new, 0)

KNOWN_WI = {
    "basic":    basic,
    "glorot":   glorot,
    "xavier":   glorot,
    "kaiming":  kaiming,
    "he":       kaiming
}
