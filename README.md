# backpropagation
This project introduces a rudimentary approach to backpropagation with a feedforward neural network. The multilayered perceptron model is built from the ground up using only Python's standard libraries. As its loss function, the model uses mean squared error. A network object can be saved to and uploaded from json.

The relevant mathematics for the backpropogation process can be found in 3Blue1Brown's video series [Neural networks](https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&si=ZHQVZqUEgnU43_X4) and on his [website](https://www.3blue1brown.com/topics/neural-networks). The weight initialization techniques can be found on Wikipedia: [Glorot](https://en.wikipedia.org/wiki/Weight_initialization#Glorot_initialization), [He](https://en.wikipedia.org/wiki/Weight_initialization#He_initialization).

A blank Network object is created by giving it a size as a list e.g. [3, 4, 4, 2] meaning three inputs, four neurons in each hidden layer and two output neurons. The Network is also given a list of activation functions which are given to all neurons of a layer, e.g. ["tanh", "sigmoid", "linear"]. Notice that the first element of size is the number of inputs and does not create a layer of neurons to be given an activation function. With these two parameters a neural network with all weights and biases of zero is created.

To initialize the weights of a Network object, use the initialize-method. The method takes a list of desired weight initialization methods as input, e.g. ["glorot", "glorot", "he"] and applies them to their respective layers. It is recommended to use Glorot (alternatively Xavier) to layers with sigmoid and tanh activation functions and He (alternatively Kaiming) to layers with relu, lrelu and linear activation functions.
